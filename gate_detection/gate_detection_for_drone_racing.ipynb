{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gate_detection_for_drone_racing.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4ckifWv54oagzoMuDUXZq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hNgDe0tMOllb"},"source":["from google.colab import drive\n","from __future__ import print_function, division\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchsummary import summary\n","import time\n","\n","# mount project dir\n","drive.mount('/content/gdrive')\n","# !ls '/content/gdrive/My Drive/'\n","path = '/content/gdrive/MyDrive/'\n","project_path = path + 'LR Research/Coding/notebooks-Colab/gate_detection/'\n","gate_path = path + 'TU Delft/Brightspace Courses/AE4317/Washington_OB_Race/Gate_Imgs/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mR4Hx__cYKpw"},"source":["We are interested in detecting the next closest gate in the track, but the most important is to detect the flyable area that allows the drone to cross the gate. For this task, the bounding box should not enclose the entire gate; instead detects the inner area of the gate. Thus the SSD network is selected to perform this objective due to its high accuracy reported; we use a reduced variant named SSD7 that has only seven convolutional layers.\n","\n","AlexNet and GoogLeNet [13], [14] showed excellent speed and performance in the ImageNet competition, and algorithms such as SSD and YOLO [4], [15] showed good performance in the detection part.\n","\n","However, these algorithms were still inadequate in terms of the computing power requirement imposed on an embedded system.\n","\n","When we overlay a single channel of our target (or prediction), we refer to this as a *mask* which illuminates the regions of an image where a specific class is present.\n","\n","Ronneberger et al. (U-Net paper) discuss a loss weighting scheme for each pixel such that there is a higher weight at the border of segmented objects."]},{"cell_type":"code","metadata":{"id":"Sl2KiSCkdwLp"},"source":["# get the CSV and the annotations\n","corner_landmarks = pd.read_csv(gate_path + 'corners.csv', header=None)\n","print(corner_landmarks, '\\n')\n","# pick an image\n","n = 200\n","img_name = corner_landmarks.iloc[n, 0]\n","img_corners = corner_landmarks.iloc[n, 1:].to_numpy()\n","# get all corners from this image\n","while corner_landmarks.iloc[n, 0] == corner_landmarks.iloc[n+1, 0]:\n","    corner_to_append = corner_landmarks.iloc[n+1, 1:].to_numpy()\n","    img_corners = np.append(img_corners, corner_to_append)\n","    n += 1\n","\n","img_corners = img_corners.astype('float').reshape(-1, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lduhuSL1aXTS"},"source":["# a helper function to show an image and its corners\n","def show_landmarks(img, corners):\n","    plt.axis('off')\n","    plt.imshow(img)\n","    plt.scatter(corners[:, 0], corners[:, 1], s=100, marker='.', c='r')\n","    \n","show_landmarks(io.imread(os.path.join(gate_path, img_name)), img_corners)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcNnQQxShhrI"},"source":["## The Gate Dataset\n","\n","`torch.utils.data.Dataset` is an abstract class representing a dataset. The `GateDataset` should \n","inherit `Dataset` and override the \n","following methods:\n","+ `__len__` so that `len(dataset)` returns the size of the dataset.\n","+ `__getitem__` to support the indexing such that `dataset[i]` can be used to get $i$th sample.\n"]},{"cell_type":"code","metadata":{"id":"YMp4UKJaxzc1"},"source":["class GateDataset(Dataset):\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.gate_corner_frames = pd.read_csv(csv_file, header=None)\n","        self.gate_img_names = []\n","        self.gate_mask_names = []\n","        self.cnt_img = 0\n","        self.cnt_mask = 0\n","\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for filename in files:\n","                if filename[:4] == 'img_':\n","                    self.cnt_img += 1\n","                    self.gate_img_names.append(filename)\n","                    self.gate_mask_names.append('mask_' + filename[4:])\n","                elif filename[:4] == 'mask':\n","                    self.cnt_mask += 1\n","        \n","        assert self.cnt_img == self.cnt_mask == len(self.gate_mask_names) \\\n","                            == len(self.gate_img_names), \"Number of masks and imgs need to be the same\"\n","        \n","        \n","    def __len__(self):\n","        return self.cnt_img\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir, self.gate_img_names[idx])\n","        mask_name = os.path.join(self.root_dir, self.gate_mask_names[idx])\n","        img = io.imread(img_name) # read as ndarray\n","        mask = io.imread(mask_name)\n","        sample = {'img': img, 'mask': mask}\n","        \n","        # apply transform if transform is not None\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","\n","# initialzie the dataset\n","gate_dataset = GateDataset(gate_path + 'corners.csv', gate_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukcOmP-rcTQ0"},"source":["Now we have built the dataset for gates and masks, the `GateDataset` can iterate through the original dataset and get access to every image and mask."]},{"cell_type":"code","metadata":{"id":"VXgAvKI-UNnI"},"source":["# plot images and masks\n","fig = plt.figure(dpi=100)\n","\n","num_of_subs = 3\n","sample_seed = np.random.randint(low=1, high=300, size=num_of_subs)\n","\n","for id, i in enumerate(sample_seed):\n","    sample = gate_dataset[i]\n","    print(i, sample['img'].shape, sample['mask'].shape)\n","    # show img\n","    ax1 = plt.subplot(2, num_of_subs, id + 1)\n","    ax1.set_title('Sample #{}'.format(i))\n","    ax1.axis('off')\n","    ax1.imshow(sample['img'])\n","    # show mask\n","    ax2 = plt.subplot(2, num_of_subs, id + num_of_subs + 1)\n","    ax2.set_title('Sample #{}'.format(i))\n","    ax2.axis('off')\n","    ax2.imshow(sample['mask'])\n","\n","    if i == num_of_subs - 1:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnkqV7ErdxVk"},"source":["## Transforms\n","\n","Before we feed the data into our neural network, we need `transforms` to transform the data to `tensor` and make some data augmentations.\n","\n","One issue we can see from the above is that the samples are not of the same size. Most neural networks expect the image of a fixed size. Therefore, we will need to write some preprocessing code. Let's create three transforms:\n","\n","+ `Rescale`: to scale the image\n","+ `RandomCrop`: to crop from image randomly. This is data augmentation.\n","+ `ToTensor`: to convert the numpy images to torch images (**we need to swap axes**).\n","\n","We will write them as *callable classes* instead of simple functions so that parameters of the transform need not to be passed everytime it's called. For this, we just need to implement `__call__` method and if required, `__init__` method. We can then use a transform like this:\n","\n","```python\n","tsfm = Transform(params) # callable object like a function\n","transformed_sample = tsfm(sample)\n","```\n","\n","In the preceding classification tasks, we scaled images to make them fit the input shape of the model. In *semantic segmentation*, this method would require us to re-map the predicted pixel categories back to the original-size input image. It would be very difficult to do this precisely, especially in segmented regions with different semantics. To avoid this problem, **we crop the images to set dimensions and do not scale them.** Specifically, we use the random cropping method used in image augmentation to crop the same region from input images and their labels."]},{"cell_type":"markdown","metadata":{"id":"rQHs0BhE2SHh"},"source":["### Segmentation Transform\n","```python\n","class ToTensor(object):\n","    def __call__(self, image, target):\n","        image = F.to_tensor(image)\n","        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n","        return image, target\n","```"]},{"cell_type":"code","metadata":{"id":"YYbspG_mLFq5"},"source":["class Rescale():\n","    \"\"\"Rescale the image in a sample to a given size.\"\"\"\n","    def __init__(self, output_size):\n","        \"\"\"\n","        Args:\n","            output_size (tuple or int): Desired output size.\n","            If tuple, output is matched to output_size. If int,\n","            smaller of image edges is matched to output_size keeping\n","            aspect ratio the same.\n","        \"\"\"\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","    \n","    def __call__(self, sample):\n","        img, mask = sample['img'], sample['mask']\n","\n","        h, w = img.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        rescaled_img = transform.resize(img, (new_h, new_w))\n","        rescaled_mask = transform.resize(mask, (new_h, new_w))\n","\n","        return {'img': rescaled_img, 'mask': rescaled_mask}\n","\n","class CustomToTensor():\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","    def __call__(self, sample):\n","        img, mask = sample['img'], sample['mask']\n","\n","        # swap color axis\n","        # numpy image: H x W x C\n","        # torch image: C x H x W\n","        to_tensor = transforms.ToTensor()\n","        img = to_tensor(img).float()\n","        # implementing the transforms for image masks.\n","        #mask = torch.as_tensor(mask / 255).unsqueeze(0)\n","        mask = to_tensor(mask).float()\n","        return {'img': img, 'mask': mask}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSCazRVhOVy1"},"source":["## Compose Transforms\n","\n","Let's say we want to rescale the shorter side if the image to 256 and then randomly crop a square of size 224 from it. i.e. we want to compose `Rescale` and `RandomCrop` transforms. `torchvision.transforms.Compose` is a simple callable class which allows us to do this.\n","\n"]},{"cell_type":"code","metadata":{"id":"F0rsc1j2tmrG"},"source":["new_hw = 224\n","print(\"Data rescaled to new width and height:\", new_hw)\n","tsfm_dataset = GateDataset(gate_path + 'corners.csv', gate_path,\n","                           transform=transforms.Compose([Rescale((new_hw, new_hw)),\n","                                                         CustomToTensor()]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brmEIYpPMlUy"},"source":["# plot images and masks\n","fig = plt.figure(dpi=100)\n","\n","num_of_subs = 3\n","sample_seed = np.random.randint(low=1, high=308, size=num_of_subs)\n","for id, i in enumerate(sample_seed):\n","    sample = tsfm_dataset[i]\n","    print(i, sample['img'].shape, sample['mask'].shape)\n","    # show img\n","    ax1 = plt.subplot(2, num_of_subs, id + 1)\n","    ax1.set_title('Sample #{}'.format(i))\n","    ax1.axis('off')\n","    ax1.imshow(sample['img'].numpy().transpose(1, 2, 0))\n","    # show mask\n","    ax2 = plt.subplot(2, num_of_subs, id + num_of_subs + 1)\n","    ax2.set_title('Sample #{}'.format(i))\n","    ax2.axis('off')\n","    #ax2.imshow(transforms.ToPILImage()(sample['mask']))\n","    ax2.imshow(sample['mask'].squeeze(0))\n","\n","    if i == num_of_subs - 1:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INz6Ocg2t4tr"},"source":["## Iterating through the dataset by using `DataLoader`\n","\n","Let's put this all together to create a dataset with composed transforms. To summarize, every time this dataset is sampled:\n","\n"," 1. An image is read from the file on the fly\n"," 2. Transforms are applied on the read image\n"," 3. Since one of the transforms is random, data is augmentated on sampling\n","\n","However, we are losing a lot of features by using a simple `for` loop to iterate over the data. In particular, we are missing out on:\n","\n","+ Batching the data\n","+ Shuffling the data\n","+ Load the data in parallel using `multiprocessing` workers\n","\n","`torch.utils.data.DataLoader` is an iterator which provides all these features. Parameters used below should be clear. One parameter of interest is `collate_fn`. You can specify how exactly the sample need to be batched using `collate_fn`. However, default collate should work fine for most use cases."]},{"cell_type":"code","metadata":{"id":"f-504hmGPL1D"},"source":["dataloader = DataLoader(tsfm_dataset,\n","                        batch_size=64,\n","                        shuffle=True)\n","\n","# helper function to show a batch\n","def show_samples_batch(sample_batched):\n","    \"\"\"Show image and masks for a batch of samples\"\"\"\n","    imgs_batch, masks_batch = sample_batched['img'], sample_batched['mask']\n","    grid_padding = 2 # default padding = 2\n","    ax1 = plt.subplot(2,1,1)\n","    ax1.axis('off')\n","    img_grid = utils.make_grid(imgs_batch, nrow=4, padding=grid_padding)\n","    ax1.imshow(img_grid.numpy().transpose((1, 2, 0)))\n","\n","    ax2 = plt.subplot(2,1,2)\n","    mask_grid = utils.make_grid(masks_batch, nrow=4, padding=grid_padding)\n","    ax2.imshow(mask_grid.numpy().transpose((1, 2, 0)))\n","\n","for i_batch, sample_batched in enumerate(dataloader):\n","    print(i_batch, sample_batched['img'].size(), sample_batched['mask'].size())\n","    # observe 4th batch and stop\n","\n","    if i_batch == 1:\n","        plt.figure()\n","        plt.title('Batch from dataloader')\n","        show_samples_batch(sample_batched)\n","        plt.axis('off')\n","        plt.ioff()\n","        plt.show()\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DNE6sPlXEa3n"},"source":["## Transposed Convolution\n","\n","As `nn.Conv2d`, both input and kernel should be 4-D tensors.\n","\n","If the stride is $s$, the padding is $s/2$ (assuming padding is an integer), and the height and width of the convolution kernel are $2s$, the transposed convolution kernel will magnify both the height and width of the input by a factor of $s$.\n","\n","- Initialize upsampling to bilinear interpolation, but allow the parameters to be learned.\n","\n","- Skip net with finer upsampling - make local predictions that respect global structure. "]},{"cell_type":"code","metadata":{"id":"L_Azj_KQEe75"},"source":["X = torch.tensor([[0., 1], [2, 3]])\n","K = torch.tensor([[0., 1], [2, 3]])\n","X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n","# tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n","# tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n","tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n","tconv.weight.data = K\n","tconv(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLfw4UDBWZy5"},"source":["## Build the Fully Convolutional Neural Network\n","\n","### VGG Architecture \n","\n","During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2."]},{"cell_type":"code","metadata":{"id":"SMnue4DsWYdY"},"source":["class FCNNet(nn.Module):\n","  def __init__(self):\n","    super(FCNNet, self).__init__()\n","    self.conv1 = nn.Conv2d(3,   16,  kernel_size=3, stride=1, padding=1)\n","    self.conv2 = nn.Conv2d(16,  32,  kernel_size=3, stride=1, padding=1)\n","    self.conv3 = nn.Conv2d(32,  64,  kernel_size=3, stride=1, padding=1)\n","    self.conv4 = nn.Conv2d(64,  96,  kernel_size=3, stride=1, padding=1)\n","    self.conv5 = nn.Conv2d(96,  128, kernel_size=3, stride=1, padding=1)\n","\n","    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    self.deconv1 = nn.ConvTranspose2d(128, 96, kernel_size=2, stride=2)\n","    self.deconv2 = nn.ConvTranspose2d(96,  64, kernel_size=2, stride=2)\n","    self.deconv3 = nn.ConvTranspose2d(64,  32, kernel_size=2, stride=2)\n","    self.deconv4 = nn.ConvTranspose2d(32,  16, kernel_size=2, stride=2)\n","\n","    self.classifier = nn.Conv2d(16, 1, kernel_size=1) # num_class = 1\n","\n","  def forward(self, x):\n","    # subsampling\n","    out = F.relu(self.conv1(x)) \n","    out = F.relu(self.maxpool(self.conv2(out)))\n","    out = F.relu(self.maxpool(self.conv3(out)))\n","    out = F.relu(self.maxpool(self.conv4(out)))\n","    out = F.relu(self.maxpool(self.conv5(out)))\n","    \n","    # upsampling\n","    out = F.relu(self.deconv1(out))\n","    out = F.relu(self.deconv2(out))\n","    out = F.relu(self.deconv3(out))\n","    out = F.relu(self.deconv4(out))\n","    score = self.classifier(out)\n","\n","    return torch.sigmoid(score)\n","\n","summary(FCNNet().cuda(), (3,224,224))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8m5omiKyefs"},"source":["## Test BCE Criterion and FCN Model\n","\n","Test a random batch, loss should *decrease*."]},{"cell_type":"code","metadata":{"id":"ematLg-4yakQ"},"source":["# test a random batch, loss should decrease\n","batch_size, img_channel, n_class, h, w = 1, 3, 1, 224, 224\n","test_model = FCNNet()\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.SGD(test_model.parameters(), lr=1e-2, momentum=0.9)\n","input = torch.randn(batch_size, img_channel, h, w)\n","\n","# before training\n","print(test_model(input)[0, 0])\n","with torch.no_grad():\n","    y = torch.ones(batch_size, n_class, h, w)\n","print(y[0, 0])\n","\n","for iter in range(50):\n","    optimizer.zero_grad()\n","    output = test_model(input)\n","    loss = criterion(output, y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if iter % 10 == 0:\n","        print(\"iter{}: loss {}\".format(iter, loss.data))\n","\n","# after training\n","print(test_model(input)[0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hn7LGpAZVp2s"},"source":["## Training Model\n","\n","Load transformed dataset consisting of gate images and masks and train the FCN model."]},{"cell_type":"code","metadata":{"id":"fbQ4aGiRV15d"},"source":["if torch.cuda.is_available():\n","    fcn_model = FCNNet().cuda()\n","\n","loss_fn = nn.BCELoss().cuda()\n","optimizer = torch.optim.SGD(fcn_model.parameters(), lr=1e-3, momentum=0.9)\n","epochs = 500\n","\n","model_path = os.path.join(project_path, 'pretrained_models/fcn32s.pt')\n","\n","def train():\n","    for epoch in range(epochs):\n","        ts = time.time()\n","        for iter, batch in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            if torch.cuda.is_available():\n","                inputs = batch['img'].cuda()\n","                labels = batch['mask'].cuda()\n","\n","            outputs = fcn_model(inputs)\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if iter % 10 == 0:\n","                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.data))\n","        \n","        #print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n","\n","    torch.save(fcn_model, model_path)\n","\n","if __name__ == '__main__': \n","    train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TG5-re1Abc7U"},"source":["test_input = tsfm_dataset[4]['img'].unsqueeze(0)\n","#test_input = torch.randn(1, 3, 224, 224)\n","test_output = fcn_model.cpu()(test_input)\n","print(test_input.shape, test_output.shape)\n","plt.imshow(test_input.squeeze(0).squeeze(0).detach().numpy().transpose(1,2,0))"],"execution_count":null,"outputs":[]}]}